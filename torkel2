#! /usr/bin/env python
from __future__ import print_function

import os
import sys
import os

import itertools
import argparse
import errno
import math

# import pickle
import dill as pickle 
import gffutils
import pysam

from collections import defaultdict


from modules import create_splice_graph as splice_graph
from modules import create_augmented_gene as augmented_gene 
from modules import mummer_wrapper 
from modules import colinear_solver 
from modules import graph_chainer 
from modules import help_functions
from modules import classify_read_with_mams
from modules import classify_alignment2
from modules import sam_output


def pickle_dump(data, filename):
    with open(os.path.join(args.outfolder,filename), 'wb') as f:
        # Pickle the 'data' dictionary using the highest protocol available.
        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)


def pickle_load(filename):
    with open(filename, 'rb') as f:
        # The protocol version used is detected automatically, so we do not
        # have to specify it.
        data = pickle.load(f)
    return data


def construct_graph(args):
    fn = gffutils.example_filename(args.gff)
    db = gffutils.create_db(fn, dbfn='test.db', force=True, keep_order=True, merge_strategy='merge', sort_attribute_values=True)
    db = gffutils.FeatureDB('test.db', keep_order=True)

    genes_to_ref, exons_to_ref, parts_to_exons, splices_to_transcripts, transcripts_to_splices, all_splice_pairs_annotations, all_splice_sites_annotations, exon_id_to_choordinates = augmented_gene.create_graph_from_exon_parts(db, args.min_mem )
    refs = {acc : seq for acc, (seq, _) in help_functions.readfq(open(args.ref,"r"))}
    ref_part_sequences = augmented_gene.get_part_sequences_from_choordinates(parts_to_exons, genes_to_ref, refs)
    ref_exon_sequences = augmented_gene.get_exon_sequences_from_choordinates(exon_id_to_choordinates, exons_to_ref, refs)

    # gene_graphs, splices_to_transcripts, annotated_transcripts = splice_graph.create_graph(db)
    # topological_sorts = splice_graph.create_global_source_sink(gene_graphs)    
    # paths = splice_graph.derive_path_cover(gene_graphs, topological_sorts)
    # ref_part_sequences = splice_graph.get_sequences_from_choordinates(gene_graphs, args.ref)
    # pickle_dump(topological_sorts, 'top_sorts.pickle')
    # pickle_dump(paths, 'paths.pickle')

    # dump to pickle here! Both graph and reference seqs
    pickle_dump(genes_to_ref, 'genes_to_ref.pickle')
    pickle_dump(exon_id_to_choordinates, 'exon_id_to_choordinates.pickle')
    pickle_dump(ref_part_sequences, 'ref_part_sequences.pickle')
    pickle_dump(ref_exon_sequences, 'ref_exon_sequences.pickle')
    pickle_dump(splices_to_transcripts, 'splices_to_transcripts.pickle')
    pickle_dump(transcripts_to_splices, 'transcripts_to_splices.pickle')
    pickle_dump(parts_to_exons, 'parts_to_exons.pickle')
    pickle_dump(all_splice_pairs_annotations, 'all_splice_pairs_annotations.pickle')
    pickle_dump(all_splice_sites_annotations, 'all_splice_sites_annotations.pickle')


# def check_if_present_in_database(solution, splices_to_transcripts):
#     for mem in solution:
#         pass



def eprint(*args, **kwargs):
    print(*args, file=sys.stderr, **kwargs)

def align_reads(args):
    # topological_sorts = pickle_load( os.path.join(args.outfolder, 'top_sorts.pickle') )
    # path_covers = pickle_load( os.path.join(args.outfolder, 'paths.pickle') )

    genes_to_ref = pickle_load( os.path.join(args.outfolder, 'genes_to_ref.pickle') )
    parts_to_exons = pickle_load( os.path.join(args.outfolder, 'parts_to_exons.pickle') )
    exon_id_to_choordinates = pickle_load( os.path.join(args.outfolder, 'exon_id_to_choordinates.pickle') )

    ref_part_sequences = pickle_load( os.path.join(args.outfolder, 'ref_part_sequences.pickle') )
    ref_exon_sequences = pickle_load( os.path.join(args.outfolder, 'ref_exon_sequences.pickle') )
    splices_to_transcripts = pickle_load( os.path.join(args.outfolder, 'splices_to_transcripts.pickle') )
    transcripts_to_splices = pickle_load( os.path.join(args.outfolder, 'transcripts_to_splices.pickle') )
    all_splice_pairs_annotations = pickle_load( os.path.join(args.outfolder, 'all_splice_pairs_annotations.pickle') )
    all_splice_sites_annotations = pickle_load( os.path.join(args.outfolder, 'all_splice_sites_annotations.pickle') )


    ref_path = os.path.join(args.outfolder, "refs_sequences.fa")
    mummer_out_path =  os.path.join( args.outfolder, "mummer_mems.txt" )
    mummer_wrapper.find_mems(args.outfolder, ref_part_sequences, args.reads, ref_path, mummer_out_path, args.min_mem)
    reads = { acc : seq for acc, (seq, qual) in help_functions.readfq(open(args.reads, 'r'))}
    mems = mummer_wrapper.parse_results(mummer_out_path)


    mems_rc = []
    if not args.ignore_rc:
        reads_rc = open(os.path.join(args.outfolder, 'reads_rc.fq'), 'w')
        for acc, seq in reads.items():
            reads_rc.write('>{0}\n{1}\n'.format(acc, help_functions.reverse_complement(seq)))
        reads_rc.close()
        args.reads_rc = reads_rc.name
        ref_path = os.path.join(args.outfolder, "refs_sequences_rc.fa")
        mummer_out_path =  os.path.join(args.outfolder, "mummer_mems_rc.txt" )
        mummer_wrapper.find_mems(args.outfolder, ref_part_sequences, args.reads_rc, ref_path, mummer_out_path, args.min_mem)
        mems_rc = mummer_wrapper.parse_results(mummer_out_path)


    
    refs = { acc : seq for acc, (seq, qual) in help_functions.readfq(open(args.ref, 'r'))} 
    refs_lengths = { acc : len(seq) for acc, seq in refs.items()} 
    alignment_outfile = pysam.AlignmentFile( os.path.join(args.outfolder, "torkel.sam"), "w", reference_names=list(refs_lengths.keys()), reference_lengths=list(refs_lengths.values()) ) #, template=samfile)
    print(alignment_outfile.header)
    # print(mems)
    # print(ref_part_sequences)
    # print(parts_to_exons)
    # print(parts_to_transcript_annotations)
    # print(all_splice_pairs_annotations)
    classifications = defaultdict(str)
    read_accessions_with_mappings = set()
    for read_acc in mems:
        print()
        print(read_acc)
        
        all_hits = []
        for chr_id, all_mems_to_chromosome in mems[read_acc].items():
            tot_aligned_sum_of_mems = sum([mem_tmp[4] for mem_tmp in all_mems_to_chromosome])
            all_hits.append( (chr_id, all_mems_to_chromosome, tot_aligned_sum_of_mems, False) )

        for chr_id, all_mems_to_chromosome in mems_rc[read_acc].items():
            tot_aligned_sum_of_mems = sum([mem_tmp[4] for mem_tmp in all_mems_to_chromosome])
            all_hits.append( (chr_id, all_mems_to_chromosome, tot_aligned_sum_of_mems, True) )

        if not all_hits:
            sam_output.main(read_acc, '*', 'unaligned', [], '*', '*', '*', alignment_outfile, is_rc)
            continue


        all_hits = sorted(all_hits, key=lambda x: x[2], reverse=True)

        best_hit_sum_mems = all_hits[0][2]
        for chr_id, mems_to_chromosome, tot_aligned_sum_of_mems, is_rc in all_hits:
            if tot_aligned_sum_of_mems/float(best_hit_sum_mems) < args.dropoff:
                continue

            print(chr_id, tot_aligned_sum_of_mems)
            solution, value, unique = colinear_solver.read_coverage(mems_to_chromosome)
            # print(splices_to_transcripts)
            # check if matches previous annotation here!

            if is_rc:
                read_seq = help_functions.reverse_complement(reads[read_acc])
            else:
                read_seq = reads[read_acc]

            print(len(read_seq))
            print("Solution mems:", solution)
            print(read_acc, is_rc, value)

            if value > 0: #value > len(read_seq) * 0.2:
                classification, annotated_to_transcript_id, mam_value, mam_solution =  classify_read_with_mams.main(solution, refs, parts_to_exons, exon_id_to_choordinates, read_seq, args.overlap_threshold, is_rc)
                if mam_value > 0:
                    print(read_acc, "alignment to:", chr_id, "best solution val mems:", value, 'best mam value:', mam_value)

                    print()
                    print(read_acc, value, len(read_seq))
                    print(value, mam_value, len(read_seq))


                    # if 'SIRV503' not in read_acc:
                    #     assert value == mam_value == len(read_seq)
                    # if read_acc == '34_SIRV3_SIRV307':
                    #     sys.exit()

                    chained_parts_seq = []
                    prev_ref_stop = -1
                    predicted_exons = []
                    for mam in mam_solution:
                        predicted_exons.append( (mam.x, mam.y) )
                        seq = ref_exon_sequences[mam.ref_chr_id][(mam.x, mam.y)] 
                        if mam.x < prev_ref_stop:
                            chained_parts_seq.append(seq[prev_ref_stop - mam.x: ])
                            print("Overlapping exons in solution!",  mam.x, prev_ref_stop, mam)
                            sys.exit()
                        else:
                            chained_parts_seq.append(seq)
                        prev_ref_stop = mam.y

                    created_ref_seq = "".join([part for part in chained_parts_seq])
                    predicted_splices = [ (e1[1],e2[0]) for e1, e2 in zip(predicted_exons[:-1],predicted_exons[1:])]

                    print(predicted_exons)
                    print(predicted_splices)
                    read_aln, ref_aln, cigar_string, cigar_tuples = help_functions.parasail_alignment(read_seq, created_ref_seq)
                    # print('read', read_seq)
                    # print('ref',created_ref_seq)
                    print(read_aln)
                    print(ref_aln, tuple( mam.exon_id for mam in mam_solution))
                    print(cigar_string)
                    print(read_aln == ref_aln)

                    # if read_acc == '51:658|61cddec3-582e-4a20-9e53-9c7758b014fc_runid=8c239806e6f576cd17d6b7d532976b1fe830f9c6_sampleid=pcs109_sirv_mix2_LC_read=51919_ch=67_start_time=2019-04-13T03:34:20Z_strand=-_strand=+':
                    #         print(mems_rc[read_acc].items())
                    #         print('loool', mam_value, mam_solution)
                    #         sys.exit()    

                    print(classifications)
                    classification, annotated_to_transcript_id = classify_alignment2.main(chr_id, predicted_splices, splices_to_transcripts, transcripts_to_splices, all_splice_pairs_annotations, all_splice_sites_annotations)


                    classifications[read_acc] = (classification, mam_value / float(len(read_seq)))
                    print(classification)
                    if classification == 'ISM/NIC_known':
                        eprint(read_acc, 'ISM/NIC_known unique solution:', unique)
                        print(solution)
                        print(predicted_exons)
                        print(predicted_splices)
                        print(chr_id, splices_to_transcripts[chr_id])
                        print( tuple(predicted_splices) in splices_to_transcripts[chr_id])
                        # sys.exit()
                    elif classification == 'FSM':
                        eprint('FSM unique solution:', unique)
                        print(value, mam_value, len(read_seq))
                        print(solution)
                        # sys.exit()
                    


                    sam_output.main(read_acc, chr_id, classification, predicted_exons, read_aln, ref_aln, annotated_to_transcript_id, alignment_outfile, is_rc)
                    read_accessions_with_mappings.add(read_acc)
                else:
                    if read_acc not in read_accessions_with_mappings:
                        sam_output.main(read_acc, '*', 'unaligned', [], '*', '*', '*', alignment_outfile, is_rc)

            else:
                if read_acc not in read_accessions_with_mappings:
                    sam_output.main(read_acc, '*', 'unaligned', [], '*', '*', '*', alignment_outfile, is_rc)



            # if classification == 'NIC_novel':
            #     print('NIC_novel', read_acc)
            #     sys.exit()
            # if read_acc == '464:2454|2d6b14f0-3651-4799-ab9e-4852fa29b3b6_runid=8c239806e6f576cd17d6b7d532976b1fe830f9c6_sampleid=pcs109_sirv_mix2_LC_read=35720_ch=117_start_time=2019-04-12T23:50:21Z_strand=+_strand=+_rescue=1':
            #     print(read_acc)
            #     sys.exit()

                # predicted_transcript = []
                # chained_parts_seq = []
                # chained_parts_ids = []
                # prev_ref_stop = -1
                # predicted_exons = []
                # for mem in solution:
                #     ref_chr_id, ref_start, ref_stop =  mem.exon_part_id.split('_')
                #     ref_start, ref_stop = int(ref_start), int(ref_stop)
                #     predicted_transcript.append( (ref_start, ref_stop) )
                #     # print("lll",ref_chr_id, ref_start, ref_stop)
                #     seq = ref_part_sequences[ref_chr_id][(ref_start, ref_stop)]
                #     if ref_start < prev_ref_stop:
                #         chained_parts_seq.append(seq[prev_ref_stop - ref_start: ])
                #     else:
                #         chained_parts_seq.append(seq)

                #     if ref_start > prev_ref_stop + 1 : # new separate exon on the genome
                #         chained_parts_ids.append(parts_to_exons[ref_chr_id][ (ref_start, ref_stop)])
                #         predicted_exons.append((prev_ref_stop, ref_start))
                #     else: # part of an exon
                #         chained_parts_ids[-1] = chained_parts_ids[-1] ^ parts_to_exons[ref_chr_id][(ref_start, ref_stop)]  #here we get mora annotations than we want , need a function to check that an exon has all parts covered!

                #     prev_ref_stop = ref_stop

                # predicted_exons.append((prev_ref_stop, -1))

                # created_ref_seq = "".join([part for part in chained_parts_seq])
                # # predicted_transcript = tuple( mem.exon_part_id for mem in solution)
                # # print( "predicted:", predicted_transcript)
                # print(predicted_exons)
                # predicted_exons = [ (e1[1],e2[0]) for (e1, e2) in zip(predicted_exons[:-1], predicted_exons[1:] )]
                # predicted_splices = [ (e1[1],e2[0]) for e1, e2 in zip(predicted_exons[:-1],predicted_exons[1:])]

                # print(predicted_exons)
                # print(predicted_splices)


                # read_aln, ref_aln, cigar_string, cigar_tuples = help_functions.parasail_alignment(read_seq, created_ref_seq)
                # # print('read', read_seq)
                # # print('ref',created_ref_seq)
                # print(read_aln)
                # print(ref_aln, tuple( mem.exon_part_id for mem in solution))
                # print(cigar_string)
                # print(read_aln == ref_aln)
                # # print([parts_to_exons[ mem.exon_part_id] for mem in solution])
                # print(chained_parts_ids)



            #     classification, annotated_to_transcript_id = classify_alignment.main(chr_id, predicted_splices, predicted_transcript, splices_to_transcripts, parts_to_transcript_annotations, transcripts_to_parts_annotations, all_splice_pairs_annotations, all_splice_sites_annotations)
            #     classifications[read_acc] = (classification, value / float(len(read_seq)))
            #     print(classification)
            #     if classification == 'NIC_comb':
            #         eprint(read_acc, 'NIC_comb unique solution:', unique)
            #         print(solution)
            #         print(parts_to_exons[ref_chr_id])
            #         print(predicted_exons)
            #         print(predicted_splices)
            #         print(chr_id, splices_to_transcripts[chr_id])
            #         print( tuple(predicted_splices) in splices_to_transcripts[chr_id])
            #         print(transcripts_to_parts_annotations[chr_id]['SIRV308'])
            #         # sys.exit()
            #     elif classification == 'FSM':
            #         eprint('FSM unique solution:', unique)
            #         print(solution)
            #         # sys.exit()

            #     sam_output.main(read_acc, chr_id, classification, predicted_exons, read_aln, ref_aln, annotated_to_transcript_id, alignment_outfile)

            # else: 
            #     sam_output.main(read_acc, '*', 'unaligned', predicted_exons, read_aln, ref_aln, annotated_to_transcript_id, alignment_outfile)

                # Write alignment to SAM file

            # if read_acc == 'read_several_optimal_matchings_1001_1186_1469_1534':
            #     sys.exit()

                # check_if_present_in_database(solution, parts_to_transcript_annotations[chr_id])

            # parasail_align_to_best(read, concatenated_exon_parts)

    alignment_outfile.close()

    print(classifications)
    counts = defaultdict(int)
    alignment_coverage = 0
    for read_acc in reads:
        if read_acc not in classifications:
            print(read_acc, "did not meet the threshold")
        elif classifications[read_acc][0] != 'FSM':
            print(read_acc, classifications[read_acc]) 

        if read_acc in classifications:
            alignment_coverage += classifications[read_acc][1]
            if classifications[read_acc][1] < 1.0:
                print(read_acc, 'alignemnt coverage:', classifications[read_acc][1])
        
            counts[classifications[read_acc][0]] += 1
        else:
            counts['unaligned'] += 1


    print(counts)
    print("total alignmenrt coverage:", alignment_coverage)
    # for gene_id in gene_graphs:
    #     gene_graph = gene_graphs[gene_id]
    #     topological_sort = topological_sorts[gene_id]
    #     path_cover = path_covers[gene_id]
    #     graph_chainer.calc_last2reach(gene_graph, path_cover, topological_sort)
    #     sys.exit()
    # non_redundant_segments = splice_graph.collapse_identical_for_mumer(exon_db)

    # exon_db = parse_tsv()
    # graph = create_graph()
    # graph_top_sorted_order = top_sort_graph()
    # hits = run_mummer(non_redundant_segments, reads )
    # for read_hits in hits:
    #     chain_hits_read()



if __name__ == '__main__':

    parser = argparse.ArgumentParser(description="torkel -- Classify long reads based on transcript parts graphs")
    subparsers = parser.add_subparsers(help='Subcommands for eaither constructing a graph, or align reads')
    # parser.add_argument("-v", help='Different subcommands for eaither constructing a graph, or align reads')

    pipeline_parser = subparsers.add_parser('pipeline', help= "Construct a splicing graph and align reads to it")
    construct_graph_parser = subparsers.add_parser('construct', help= "Construct a splicing graph")
    align_reads_parser = subparsers.add_parser('align', help="Classify and align reads with colinear chaining to DAGs")

    pipeline_parser.add_argument('gff', type=str, help='Path to gff or gtf file with gene models.')
    pipeline_parser.add_argument('ref', type=str, help='Reference genome (fasta)')
    pipeline_parser.add_argument('reads', type=str, help='Path to fasta/fastq file with reads.')
    pipeline_parser.add_argument('--min_mem', type=int, default=16, help='Threchold for what is counted as varation/intron in alignment as opposed to deletion.')
    pipeline_parser.add_argument('--overlap_threshold', type=int, default=10, help='Threshold for what is counted as varation/intron in alignment as opposed to deletion.')
    pipeline_parser.add_argument('--dropoff', type=float, default=0.8, help='Ignore alignment to hits with read coverage of this fraction less than the best hit.')
    pipeline_parser.add_argument('--ignore_rc', action='store_true', help='Ignore to map to reverse complement.')
    pipeline_parser.add_argument('outfolder', type=str, help='Path to fasta file with a nucleotide sequence (e.g., gene locus) to simulate isoforms from.')
    # pipeline_parser.add_argument('--pickle', action='store_true', help='Store the graph on file.')
    pipeline_parser.set_defaults(which='pipeline')


    construct_graph_parser.add_argument('gff', type=str, help='Path to gff or gtf file with gene models.')
    construct_graph_parser.add_argument('ref', type=str, help='Reference genome (fasta)')
    construct_graph_parser.add_argument('outfolder', type=str, help='Path to fasta file with a nucleotide sequence (e.g., gene locus) to simulate isoforms from.')
    construct_graph_parser.add_argument('--min_mem', type=int, default=16, help='Threchold for what is counted as varation/intron in alignment as opposed to deletion.')
    # construct_graph_parser.add_argument('--pickle', action='store_true', help='Store the graph on file.')
    construct_graph_parser.set_defaults(which='construct_graph')


    align_reads_parser.add_argument('refs', type=str, help='Path to fasta file with a nucleotide sequence (e.g., gene locus) to simulate isoforms from.')    
    align_reads_parser.add_argument('reads', type=str, help='Path to fasta file with a nucleotide sequence (e.g., gene locus) to simulate isoforms from.')
    align_reads_parser.add_argument('outfolder', type=str, help='Path to fasta file with a nucleotide sequence (e.g., gene locus) to simulate isoforms from.')   
    align_reads_parser.add_argument('--overlap_threshold', type=int, default=10, help='Threchold for what is counted as varation/intron in alignment as opposed to deletion.')
    align_reads_parser.add_argument('--dropoff', type=float, default=0.8, help='Ignore alignment to hits with read coverage of this fraction less than the best hit.')
    align_reads_parser.add_argument('--ignore_rc', action='store_true', help='Ignore to map to reverse complement.')
    align_reads_parser.add_argument('--min_mem', type=int, default=16, help='Threchold for what is counted as varation/intron in alignment as opposed to deletion.')


    align_reads_parser.set_defaults(which='align_reads')

    args = parser.parse_args()


    if len(sys.argv)==1:
        parser.print_help()
        sys.exit()

    if args.which == 'construct_graph':
        construct_graph(args)
    elif args.which == 'align_reads':
        align_reads(args)
    elif args.which == 'pipeline':
        construct_graph(args)
        align_reads(args)        
    else:
        print('invalid call')
