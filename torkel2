#! /usr/bin/env python
from __future__ import print_function

import os
import sys
import os

import itertools
from itertools import islice, chain

import argparse
import errno
import math

# import pickle
import dill as pickle 
import gffutils
import pysam


from collections import defaultdict


# from modules import create_splice_graph as splice_graph
# from modules import graph_chainer 

from modules import create_augmented_gene as augmented_gene 
from modules import mummer_wrapper 
from modules import colinear_solver 
from modules import help_functions
from modules import classify_read_with_mams
from modules import classify_alignment2
from modules import sam_output
from modules import align





def prep_splicing(args):
    database = os.path.join(args.outfolder,'database.db')
    if os.path.isfile(database):
        print("Database found in directory using this one.")
        print("If you want to recreate the database, please remove the file: {0}".format(database))
        print()
        db = gffutils.FeatureDB(database, keep_order=True)
        # sys.exit()
    elif not args.disable_infer:
        fn = gffutils.example_filename(args.gtf)
        db = gffutils.create_db(fn, dbfn=database, force=True, keep_order=True, merge_strategy='merge', 
                                sort_attribute_values=True)
        db = gffutils.FeatureDB(database, keep_order=True)
    else:
        fn = gffutils.example_filename(args.gtf)
        db = gffutils.create_db(fn, dbfn=database, force=True, keep_order=True, merge_strategy='merge', 
                                sort_attribute_values=True, disable_infer_genes=True, disable_infer_transcripts=True)
        db = gffutils.FeatureDB(database, keep_order=True)

    exons_to_ref, parts_to_exons, splices_to_transcripts, transcripts_to_splices, all_splice_pairs_annotations, all_splice_sites_annotations, exon_id_to_choordinates = augmented_gene.create_graph_from_exon_parts(db, args.min_mem )

    # dump to pickle here! Both graph and reference seqs
    # help_functions.pickle_dump(args, genes_to_ref, 'genes_to_ref.pickle')
    help_functions.pickle_dump(args, exons_to_ref, 'exons_to_ref.pickle')
    help_functions.pickle_dump(args, splices_to_transcripts, 'splices_to_transcripts.pickle')
    help_functions.pickle_dump(args, transcripts_to_splices, 'transcripts_to_splices.pickle')
    help_functions.pickle_dump(args, parts_to_exons, 'parts_to_exons.pickle')
    help_functions.pickle_dump(args, all_splice_pairs_annotations, 'all_splice_pairs_annotations.pickle')
    help_functions.pickle_dump(args, all_splice_sites_annotations, 'all_splice_sites_annotations.pickle')
    help_functions.pickle_dump(args, exon_id_to_choordinates, 'exon_id_to_choordinates.pickle')


def prep_seqs(args):
    # genes_to_ref = help_functions.pickle_load( os.path.join(args.outfolder, 'genes_to_ref.pickle') )
    parts_to_exons = help_functions.pickle_load( os.path.join(args.outfolder, 'parts_to_exons.pickle') )
    exon_id_to_choordinates = help_functions.pickle_load( os.path.join(args.outfolder, 'exon_id_to_choordinates.pickle') )
    exons_to_ref = help_functions.pickle_load( os.path.join(args.outfolder, 'exons_to_ref.pickle') )
    print(parts_to_exons.keys())
    # print(genes_to_ref.keys())
    refs = {acc : seq for acc, (seq, _) in help_functions.readfq(open(args.ref,"r"))}
    print(refs.keys())
    ref_part_sequences = augmented_gene.get_part_sequences_from_choordinates(parts_to_exons, refs)
    ref_exon_sequences = augmented_gene.get_exon_sequences_from_choordinates(exon_id_to_choordinates, exons_to_ref, refs)

    help_functions.pickle_dump(args, exon_id_to_choordinates, 'exon_id_to_choordinates.pickle')
    help_functions.pickle_dump(args, ref_part_sequences, 'ref_part_sequences.pickle')
    help_functions.pickle_dump(args, ref_exon_sequences, 'ref_exon_sequences.pickle')

    # gene_graphs, splices_to_transcripts, annotated_transcripts = splice_graph.create_graph(db)
    # topological_sorts = splice_graph.create_global_source_sink(gene_graphs)    
    # paths = splice_graph.derive_path_cover(gene_graphs, topological_sorts)
    # ref_part_sequences = splice_graph.get_sequences_from_choordinates(gene_graphs, args.ref)
    # help_functions.pickle_dump(args, topological_sorts, 'top_sorts.pickle')
    # help_functions.pickle_dump(args, paths, 'paths.pickle')


def import_data(args):
    exon_id_to_choordinates = help_functions.pickle_load( os.path.join(args.outfolder, 'exon_id_to_choordinates.pickle'))
    ref_exon_sequences = help_functions.pickle_load( os.path.join(args.outfolder, 'ref_exon_sequences.pickle') )
    splices_to_transcripts = help_functions.pickle_load( os.path.join(args.outfolder, 'splices_to_transcripts.pickle') )
    transcripts_to_splices = help_functions.pickle_load( os.path.join(args.outfolder, 'transcripts_to_splices.pickle') )
    all_splice_pairs_annotations = help_functions.pickle_load( os.path.join(args.outfolder, 'all_splice_pairs_annotations.pickle') )
    all_splice_sites_annotations = help_functions.pickle_load( os.path.join(args.outfolder, 'all_splice_sites_annotations.pickle') )
    parts_to_exons = help_functions.pickle_load( os.path.join(args.outfolder, 'parts_to_exons.pickle') )
    return exon_id_to_choordinates, ref_exon_sequences, splices_to_transcripts, transcripts_to_splices, all_splice_pairs_annotations, all_splice_sites_annotations, parts_to_exons


def batch(dictionary, size):
    batches = []
    sub_dict = {}
    for i, (acc, seq) in enumerate(dictionary.items()):
        if i > 0 and i % size == 0:
            batches.append(sub_dict)
            sub_dict = {}
            sub_dict[acc] = seq
        else:
            sub_dict[acc] = seq

    if i/size != 0:
        sub_dict[acc] = seq
        batches.append(sub_dict)
    
    return batches

    # sourceiter = iter(dictionary)
    # while True:
    #     batchiter = islice(sourceiter, size)
    #     try:
    #         yield list(chain([next(batchiter)], batchiter))
    #     except StopIteration:
    #         return


def align_reads(args):
    # topological_sorts = help_functions.pickle_load( os.path.join(args.outfolder, 'top_sorts.pickle') )
    # path_covers = help_functions.pickle_load( os.path.join(args.outfolder, 'paths.pickle') )

    ref_part_sequences = help_functions.pickle_load( os.path.join(args.outfolder, 'ref_part_sequences.pickle') )

    ref_path = os.path.join(args.outfolder, "refs_sequences.fa")
    refs_file = open(ref_path, 'w') #open(os.path.join(outfolder, "refs_sequences_tmp.fa"), "w")
    for chr_id  in ref_part_sequences:
        for (start,stop), seq  in ref_part_sequences[chr_id].items():
            refs_file.write(">{0}\n{1}\n".format(chr_id + str("_") + str(start) + "_" + str(stop), seq))
    refs_file.close()

    mummer_out_path =  os.path.join( args.outfolder, "mummer_mems.txt" )
    if os.path.isfile(mummer_out_path):
        pass
    else:
        print("Running mummer forward")
        mummer_wrapper.find_mems(args.outfolder, ref_part_sequences, args.reads, ref_path, mummer_out_path, args.min_mem)
        print("Completed mummer forward")
    # print("Parsing mems")
    # mems = mummer_wrapper.parse_results(mummer_out_path)
    # print("Completed parsing mems")

    mems_rc = []
    # if not args.ignore_rc:
    print("Processing reverse complement reads for mummer reads")
    reads_rc = open(os.path.join(args.outfolder, 'reads_rc.fq'), 'w')
    for acc, (seq, qual) in help_functions.readfq(open(args.reads, 'r')):
        reads_rc.write('>{0}\n{1}\n'.format(acc, help_functions.reverse_complement(seq)))
    reads_rc.close()
    print("Finished processing reverse complement reads for mummer reads")

    args.reads_rc = reads_rc.name
    # ref_path = os.path.join(args.outfolder, "refs_sequences_rc.fa")
    mummer_out_path =  os.path.join(args.outfolder, "mummer_mems_rc.txt" )
    if os.path.isfile(mummer_out_path):
        pass
    else:
        print("Starting mummer reverse")
        mummer_wrapper.find_mems(args.outfolder, ref_part_sequences, args.reads_rc, ref_path, mummer_out_path, args.min_mem)
        print("Completed mummer reverse")
    # print("Parsing rev comp mems")
    # mems_rc = mummer_wrapper.parse_results(mummer_out_path)
    # print("Finished parsing rev comp mems")

    print("Read reference")
    refs = { acc : seq for acc, (seq, qual) in help_functions.readfq(open(args.ref, 'r'))} 
    refs_lengths = { acc : len(seq) for acc, seq in refs.items()} 
    del refs
 
    print("Completed reading reference")

    reads = { acc : seq for acc, (seq, qual) in help_functions.readfq(open(args.reads, 'r'))}

    print("Starting aligning reads.")
    auxillary_data = import_data(args)
    if args.nr_cores == 1:
        # print("ENTER")
        # print(alignment_outfile.header)
        # read_data = [ (acc, reads[acc], mems[acc], mems_rc[acc]) for i, acc in enumerate(mems)]
        # del reads
        # del mems
        # del mems_rc
        # sys.exit()
        classifications, alignment_outfile_name = align.align_single(reads, auxillary_data, refs_lengths, args, -1)
    else:
        # batch reads and mems up: divide reads by  nr_cores to get batch size
        # then write to separate SAM-files with a batch index, 
        # finally merge sam file by simple cat in terminal 
        
        batch_size = int(len(reads)/int(args.nr_cores) + 1)
        read_batches = batch(reads, batch_size)
        # print(batch_size, len(read_batches))
        # for b in read_batches:
        #     print(len(b), b)
        # sys.exit()
        # read_data = batch([ (acc, reads[acc], mems[acc], mems_rc[acc]) for i, acc in enumerate(mems)], batch_size)
        print('Nr reads:', len(reads), 'Batch size:', batch_size)
        # del reads
        # del mems
        # del mems_rc
        classifications, alignment_outfiles = align.align_parallel(read_batches, auxillary_data, refs_lengths, args)

        # Combine samfiles produced from each batch
        alignment_outfile = pysam.AlignmentFile( os.path.join(args.outfolder, "torkel.sam"), "w", reference_names=list(refs_lengths.keys()), reference_lengths=list(refs_lengths.values()) ) #, template=samfile)
    
        for f in alignment_outfiles:
            samfile = pysam.AlignmentFile(f, "rb")
            for read in samfile.fetch():
                alignment_outfile.write(read)
            samfile.close()

        alignment_outfile.close()


    counts = defaultdict(int)
    alignment_coverage = 0
    for read_acc in reads:
        if read_acc not in classifications:
            # print(read_acc, "did not meet the threshold")
            pass
        elif classifications[read_acc][0] != 'FSM':
            # print(read_acc, classifications[read_acc]) 
            pass
        if read_acc in classifications:
            alignment_coverage += classifications[read_acc][1]
            if classifications[read_acc][1] < 1.0:
                # print(read_acc, 'alignemnt coverage:', classifications[read_acc][1])
                pass
            counts[classifications[read_acc][0]] += 1
        else:
            counts['unaligned'] += 1


    print(counts)
    print("total alignment coverage:", alignment_coverage)


if __name__ == '__main__':

    parser = argparse.ArgumentParser(description="torkel -- Classify long reads based on transcript parts graphs")
    subparsers = parser.add_subparsers(help='Subcommands for eaither constructing a graph, or align reads')
    # parser.add_argument("-v", help='Different subcommands for eaither constructing a graph, or align reads')

    pipeline_parser = subparsers.add_parser('pipeline', help= "Perform all in one: prepare splicing database and reference sequences and align reads.")
    prep_splicing_parser = subparsers.add_parser('prep_splicing', help= "Prepare all splicing structures from annotation")
    prep_seqs_parser = subparsers.add_parser('prep_seqs', help= "Prepare reference sequences to align to.")
    align_reads_parser = subparsers.add_parser('align', help="Classify and align reads with colinear chaining to DAGs")

    pipeline_parser.add_argument('gtf', type=str, help='Path to gtf or gtf file with gene models.')
    pipeline_parser.add_argument('ref', type=str, help='Reference genome (fasta)')
    pipeline_parser.add_argument('reads', type=str, help='Path to fasta/fastq file with reads.')
    pipeline_parser.add_argument('outfolder', type=str, help='Path to fasta file with a nucleotide sequence (e.g., gene locus) to simulate isoforms from.')
    pipeline_parser.add_argument('--min_mem', type=int, default=16, help='Threchold for what is counted as varation/intron in alignment as opposed to deletion.')
    pipeline_parser.add_argument('--overlap_threshold', type=int, default=10, help='Threshold for what is counted as varation/intron in alignment as opposed to deletion.')
    pipeline_parser.add_argument('--t', dest = 'nr_cores', type=int, default=3, help='Number of cores.')
    pipeline_parser.add_argument('--non_covered_cutoff', type=int, default=10, help='Threshold for what is counted as varation/intron in alignment as opposed to deletion.')
    pipeline_parser.add_argument('--dropoff', type=float, default=0.7, help='Ignore alignment to hits with read coverage of this fraction less than the best hit.')
    pipeline_parser.add_argument('--ignore_rc', action='store_true', help='Ignore to map to reverse complement.')
    pipeline_parser.add_argument('--disable_infer', action='store_true', help='Makes splice creation step much faster. Thes parameter can be set if gene and transcript name fields are provided in gtf file.')
    pipeline_parser.set_defaults(which='pipeline')


    prep_splicing_parser.add_argument('gtf', type=str, help='Path to gtf or gtf file with gene models.')
    prep_splicing_parser.add_argument('outfolder', type=str, help='Path to fasta file with a nucleotide sequence (e.g., gene locus) to simulate isoforms from.')
    prep_splicing_parser.add_argument('--min_mem', type=int, default=16, help='Threchold for what is counted as varation/intron in alignment as opposed to deletion.')
    prep_splicing_parser.add_argument('--disable_infer', action='store_true', help='Makes splice creation step much faster. Thes parameter can be set if gene and transcript name fields are provided in gtf file.')
    prep_splicing_parser.set_defaults(which='prep_splicing')

    prep_seqs_parser.add_argument('ref', type=str, help='Reference genome (fasta)')
    prep_seqs_parser.add_argument('outfolder', type=str, help='Path to fasta file with a nucleotide sequence (e.g., gene locus) to simulate isoforms from.')
    prep_seqs_parser.add_argument('--min_mem', type=int, default=16, help='Threchold for what is counted as varation/intron in alignment as opposed to deletion.')
    prep_seqs_parser.set_defaults(which='prep_seqs')


    align_reads_parser.add_argument('ref', type=str, help='Path to fasta file with a nucleotide sequence (e.g., gene locus) to simulate isoforms from.')    
    align_reads_parser.add_argument('reads', type=str, help='Path to fasta file with a nucleotide sequence (e.g., gene locus) to simulate isoforms from.')
    align_reads_parser.add_argument('outfolder', type=str, help='Path to fasta file with a nucleotide sequence (e.g., gene locus) to simulate isoforms from.')   
    align_reads_parser.add_argument('--t', dest = 'nr_cores', type=int, default=3, help='Number of cores.')
    align_reads_parser.add_argument('--overlap_threshold', type=int, default=10, help='Threchold for what is counted as varation/intron in alignment as opposed to deletion.')
    align_reads_parser.add_argument('--non_covered_cutoff', type=int, default=10, help='Threshold for what is counted as varation/intron in alignment as opposed to deletion.')
    align_reads_parser.add_argument('--dropoff', type=float, default=0.7, help='Ignore alignment to hits with read coverage of this fraction less than the best hit.')
    align_reads_parser.add_argument('--ignore_rc', action='store_true', help='Ignore to map to reverse complement.')
    align_reads_parser.add_argument('--min_mem', type=int, default=16, help='Threchold for what is counted as varation/intron in alignment as opposed to deletion.')


    align_reads_parser.set_defaults(which='align_reads')

    args = parser.parse_args()

    help_functions.mkdir_p(args.outfolder)
    if len(sys.argv)==1:
        parser.print_help()
        sys.exit()

    if args.which == 'prep_splicing':
        prep_splicing(args)
    elif args.which == 'prep_seqs':
        prep_seqs(args)
    elif args.which == 'align_reads':
        align_reads(args)
    elif args.which == 'pipeline':
        prep_splicing(args)
        prep_seqs(args)
        align_reads(args)        
    else:
        print('invalid call')
